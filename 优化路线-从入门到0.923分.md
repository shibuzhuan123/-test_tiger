# Jaguar ReID 竞赛优化路线：从入门到 0.923 分

> **项目背景**: 美洲豹个体识别竞赛
>
> **最终成绩**: 0.923 分 (Top 10%)
>
> **本文用途**: 保研面试技术分享

---

## 目录

1. [项目概述](#项目概述)
2. [问题分析](#问题分析)
3. [优化路线](#优化路线)
4. [技术实现](#技术实现)
5. [实验结果](#实验结果)
6. [经验总结](#经验总结)

---

## 项目概述

### 竞赛任务

**目标**: 判断两张图片中的美洲豹是否为同一只个体

**输入**: 两张美洲豹图片
**输出**: 相似度分数 (0-1 之间，1 表示同一只)

**评估指标**: 与真实标签的相关系数

### 数据规模

| 数据集 | 样本数 | 个体数 |
|--------|--------|--------|
| 训练集 | 1,895 张 | 31 个个体 |
| 测试集 | 371 张 | - |
| 测试对 | 137,271 对 | - |

---

## 问题分析

### 核心挑战

1. **类内差异大**: 同一只美洲豹在不同姿态、光照下差异很大
2. **类间差异小**: 不同美洲豹外观相似
3. **背景干扰**: 模型可能依赖背景而非主体特征
4. **数据稀缺**: 每个个体样本较少（平均 60 张）

### 解决思路

这是一个经典的 **度量学习 (Metric Learning)** 问题：

```
传统分类: 学习 "是什么" → 输出类别标签
度量学习: 学习 "像不像" → 输出特征向量
```

**ReID (Person Re-Identification) 的迁移应用**：
- 行人重识别: 识别同一个人在不同摄像头的图像
- 美洲豹识别: 识别同一只美洲豹在不同位置的照片

---

## 优化路线

### 完整优化历程

```
┌─────────────────────────────────────────────────────────────┐
│                    优化路线图                                 │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  v1 (基础版)          v2 (特征工程)        v3 (SOTA技术)    │
│  Score: ~0.70         Score: ~0.75         Score: 0.923    │
│    │                     │                     │              │
│    ▼                     ▼                     ▼              │
│  • ResNet152          • Alpha Mask         • EVA-02 Large  │
│  • 简单池化           • 数据增强           • GeM Pooling   │
│  • 固定学习率         • Label Smoothing     • 混合精度训练  │
│  • 无后处理           • 余弦退火学习率       • TTA           │
│  • Batch=64           • Mixup              • Query Exp     │
│                       • Dropout             • Re-ranking   │
│                                              • 梯度累积      │
└─────────────────────────────────────────────────────────────┘
```

---

## 技术实现

### 版本 1: 基础版本 (Score: ~0.70)

#### 架构设计

```python
class BasicModel(nn.Module):
    def __init__(self):
        # 使用 ResNet152 作为特征提取器
        self.backbone = timm.create_model('resnet152', pretrained=True)

        # 简单的全连接层
        self.neck = nn.Sequential(
            nn.BatchNorm1d(2048),
            nn.Linear(2048, 512),   # 降维
            nn.BatchNorm1d(512),
        )

        # 分类头
        self.head = ArcFaceLayer(512, num_classes)
```

#### 关键技术

1. **ResNet152**: 经典 CNN，ImageNet 预训练
2. **ArcFace Loss**: 行人识别领域常用的损失函数
3. **简单池化**: Global Average Pooling (GAP)

#### 存在问题

| 问题 | 影响 |
|------|------|
| 模型容量不足 | 特征表达受限 |
| 简单池化 | 丢失细节信息 |
| 固定学习率 | 收敛不稳定 |
| 无后处理 | 测试时没有提升空间 |

---

### 版本 2: 特征工程版本 (Score: ~0.75)

#### 改进点

1. **Alpha Mask 处理**

```python
def apply_alpha_mask(image, background='random'):
    """
    去除背景，强迫模型学习主体特征

    为什么这样做：
    - 讨论区提示模型可能依赖背景
    - 去除背景后，模型被迫学习美洲豹本身
    """
    if image.mode == 'RGBA':
        r, g, b, a = image.split()
        bg = create_background(background)  # 随机背景
        bg.paste(image.convert('RGB'), mask=a)
        return bg
```

**理论依据**：
- 模型应该学习对背景不变的表征
- 随机背景可以防止模型过拟合特定背景

2. **数据增强**

```python
transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(0.2, 0.2, 0.2),
    transforms.RandomErasing(p=0.2),
])
```

**增强策略**：
- 几何变换: 旋转、翻转、裁剪
- 颜色变换: 亮度、对比度、饱和度
- 随机遮挡: 模拟遮挡情况

3. **训练技巧**

```python
# Mixup 数据增强
def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    mixed_x = lam * x + (1 - lam) * x[perm]
    return mixed_x, y, y[perm], lam

# Label Smoothing
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

**效果**：
- Mixup: 防止过拟合，提高泛化
- Label Smoothing: 防止过度自信

#### 结果分析

| 指标 | v1 | v2 | 提升 |
|------|-----|-----|------|
| 分数 | ~0.70 | ~0.75 | +7% |
| 训练稳定性 | 中 | 良 | ↑ |
| 泛化能力 | 弱 | 强 | ↑ |

**但是**: 这个版本未能达到预期效果

**反思**：
- Alpha Mask 可能不是最优策略
- 模型架构仍然是瓶颈
- 需要从第一名代码学习

---

### 版本 3: SOTA 技术版本 (Score: 0.923)

#### 灵感来源

分析了竞赛第一名的代码，发现几个关键技术：

1. **更强的模型**: EVA-02 Large
2. **更好的池化**: GeM Pooling
3. **后处理技巧**: TTA + QE + Re-ranking

#### 核心改进

##### 改进 1: 模型升级

```python
# 从 ResNet152 → EVA-02 Large
# ResNet: CNN 局部感受野
# EVA-02: Transformer 全局感受野

class JaguarModel(nn.Module):
    def __init__(self):
        self.backbone = timm.create_model(
            'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k',
            pretrained=True
        )
```

**为什么 EVA-02 更强**：

| 特性 | ResNet152 | EVA-02 Large |
|------|-----------|---------------|
| 架构 | CNN | Vision Transformer |
| 感受野 | 局部 | 全局 |
| 预训练 | ImageNet-1K | MIM + ImageNet-22K + ImageNet-1K |
| 参数量 | 60M | 300M+ |
| 输入尺寸 | 224 | 448 |

**Vision Transformer 优势**：
- Self-Attention 机制捕捉全局依赖
- 更大的感受野
- 更强的预训练带来更好的初始化

##### 改进 2: GeM Pooling

```python
class GeM(nn.Module):
    """
    广义均值池化

    数学原理：
    GeM(x) = (1/n * Σ|xᵢ|ᵖ)^(1/p)

    当 p=1: Average Pooling
    当 p→∞: Max Pooling
    可学习的 p: 自适应调整
    """
    def __init__(self, p=3, eps=1e-6):
        super().__init__()
        self.p = nn.Parameter(torch.ones(1) * p)  # 可学习！

    def forward(self, x):
        return F.avg_pool2d(
            x.clamp(min=self.eps).pow(self.p),
            (x.size(-2), x.size(-1))
        ).pow(1.0 / self.p)
```

**为什么 GeM 更好**：

```
GAP (Global Average Pooling): 对所有值平等对待
GMP (Global Max Pooling): 只保留最大值
GeM (Generalized Mean): 可学习地平衡

可学习的参数 p 让网络自适应：
- 对某些特征通道更关注最大值
- 对其他通道更关注平均值
```

##### 改进 3: 超参数调优

| 参数 | v2 | v3 | 原因 |
|------|-----|-----|------|
| 学习率 | 1e-3 | 2e-5 | 大模型需要小学习率微调 |
| Batch Size | 128 | 8×4=32 | 梯度累积节省显存 |
| Epochs | 10 | 15 | 更充分训练 |
| 优化器 | Adam | AdamW | AdamW 解耦权重衰减 |

**学习率分析**：

```
小模型 + 从头训练 → 大学习率 (1e-3)
大模型 + 强预训练 → 小学习率 (2e-5)

原因：
- 预训练权重已经很好
- 大学习率会破坏已有知识
- 小学习率进行微调
```

##### 改进 4: 后处理技巧 (杀手锏)

###### A. TTA (Test Time Augmentation)

```python
def extract_features(model, loader):
    for imgs, fnames in loader:
        f1 = model(imgs)              # 原图
        if use_tta:
            f2 = model(torch.flip(imgs, [3]))  # 水平翻转
            f1 = (f1 + f2) / 2        # 平均
```

**原理**：
- 同一个样本，不同变换下特征应相似
- 取平均可以减少噪声
- 提升鲁棒性

**效果**: +1-2%

###### B. Query Expansion (查询扩展)

```python
def query_expansion(emb, top_k=3):
    # 1. 找到每个样本最相似的 top-k 个样本
    sims = emb @ emb.T
    indices = np.argsort(-sims, axis=1)[:, :top_k]

    # 2. 用 top-k 样本的均值作为新特征
    new_emb = np.array([np.mean(emb[indices[i]], axis=0)
                        for i in range(len(emb))])
    return new_emb / norm(new_emb)
```

**原理**：
```
原始查询: 美洲豹 A 的照片
      ↓
找到最相似的 3 张照片 (都是 A)
      ↓
用 4 张照片的特征均值作为查询
      ↓
更准确的表示！
```

**效果**: +2-3%

###### C. K-reciprocal Re-ranking

```python
def k_reciprocal_rerank(prob, k1=20, k2=6, lambda_value=0.3):
    """
    相互最近邻重排序

    核心思想：
    - 如果 A 是 B 的最近邻，且 B 是 A 的最近邻
    - 那么 A 和 B 很可能是同类
    """
```

**原理**：
- 原始排序可能有假阳性
- 考虑相互最近邻关系
- 使用 Jaccard 相似度重新排序

**效果**: +3-5%

##### 改进 5: 训练稳定性

```python
# 混合精度训练
scaler = torch.amp.GradScaler(device_type)
with torch.amp.autocast(device_type):
    loss = model(inputs)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()

# 梯度累积
for i, batch in enumerate(loader):
    loss = criterion(model(batch)) / grad_accum
    loss.backward()
    if (i + 1) % grad_accum == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**优势**：
- 混合精度: 加速 30-50%，节省显存
- 梯度累积: 有效 batch size × 4

---

## 实验结果

### 版本对比

| 版本 | 模型 | 关键技术 | 分数 | 主要改进 |
|------|------|----------|------|----------|
| v1 | ResNet152 | 基础 ReID | ~0.70 | - |
| v2 | ResNet152 | Alpha Mask + 增强 | ~0.75 | 特征工程 |
| v3 | EVA-02 | SOTA 后处理 | **0.923** | 模型+技巧 |

### 关键发现

1. **模型架构 > 特征工程**: EVA-02 比 Alpha Mask 更重要
2. **后处理至关重要**: TTA + QE + Re-rank 贡献 +5-8%
3. **超参数敏感**: 大模型需要小学习率
4. **预训练价值**: MIM + ImageNet-22K 预训练效果显著

---

## 经验总结

### 技术层面

#### 1. 模型选择的经验

```
数据规模小 (1K-10K) → 使用 ResNet, EfficientNet
数据规模中等 (10K-100K) → 使用 Swin, ConvNeXt
数据规模大 (100K+) 或 追求极致 → 使用 EVA-02, ViT
```

#### 2. 训练策略

```
小模型 → 大学习率 (1e-3) + 从头训练
大模型 → 小学习率 (1e-5) + 微调
强预训练 → Label Smoothing + Mixup
显存不足 → 梯度累积 + 混合精度
```

#### 3. 后处理优先级

```
快速提升: TTA (1-2%)
中等提升: Query Expansion (2-3%)
最大提升: Re-ranking (3-5%)
组合使用: 累积提升 5-10%
```

### 方法论层面

#### 1. 竞赛方法论

```
Step 1: 快速搭建 Baseline (1-2 天)
        ↓
Step 2: 分析第一名代码 (关键!)
        ↓
Step 3: 复现核心技巧 (2-3 天)
        ↓
Step 4: 超参数调优 (2-3 天)
        ↓
Step 5: 创新改进 (持续)
```

#### 2. 代码分析技巧

分析第一名代码时，关注：
- **模型架构**: 什么 backbone，什么池化
- **超参数**: 学习率、batch size、epochs
- **数据处理**: 增强策略、归一化参数
- **后处理**: TTA、QE、Re-ranking 等技巧
- **训练细节**: 优化器、调度器、混合精度

#### 3. 实验验证

```
理论假设 → 实验验证 → 数据分析 → 调整策略
```

例如：
- 假设: Alpha Mask 会提升性能
- 实验: 实现 v2，发现效果一般
- 分析: 第一名没用 Alpha Mask，用了更强的模型
- 调整: 换 EVA-02 + 后处理技巧
- 结果: 分数大幅提升

### 面试经验

#### 1. 准备材料

1. **代码仓库**: 整理清晰的 GitHub
2. **技术文档**: 说明每个版本的设计思路
3. **实验记录**: 记录每个实验的结果和发现
4. **对比分析**: 与其他方法的对比

#### 2. 讲述技巧

**STAR 法则**:
- **Situation**: 竞赛任务和数据特点
- **Task**: 面临的核心挑战
- **Action**: 采取的解决方案
- **Result**: 最终成果和收获

**示例**:
```
S: 美洲豹个体识别竞赛，训练集 1895 张
T: 同一个体在不同姿态下差异大
A: 使用 EVA-02 + GeM Pooling + 后处理技巧
R: 最终达到 0.923 分（Top 10%）
```

#### 3. 深入准备

可能被问的问题：
- 为什么选择 EVA-02 而不是 ResNet？
- GeM Pooling 的数学原理是什么？
- Query Expansion 为什么有效？
- 如何处理数据不足的问题？
- 如果给你更多数据，你会如何改进？

---

## 附录：核心技术清单

### 模型架构

- [x] ResNet152
- [x] EVA-02 Large
- [ ] Vision Transformer (ViT)
- [ ] Swin Transformer

### 损失函数

- [x] CrossEntropyLoss
- [x] ArcFace Loss
- [ ] Triplet Loss
- [ ] Contrastive Loss

### 池化方法

- [x] Global Average Pooling (GAP)
- [x] Global Max Pooling (GMP)
- [x] GeM Pooling

### 数据增强

- [x] 几何变换 (旋转、翻转)
- [x] 颜色变换
- [x] Mixup
- [ ] CutMix
- [ ] Mosaic

### 训练技巧

- [x] Label Smoothing
- [x] 混合精度训练 (AMP)
- [x] 梯度累积
- [x] 学习率调度 (Cosine Annealing)

### 后处理

- [x] Test Time Augmentation (TTA)
- [x] Query Expansion (QE)
- [x] K-reciprocal Re-ranking

### 优化器

- [x] SGD
- [x] Adam
- [x] AdamW
- [ ] AdaBelief

---

## 总结

### 核心收获

1. **模型的重要性**: 强模型 + 简单技巧 > 弱模型 + 复杂技巧
2. **后处理的价值**: TTA + QE + Re-rank 可以带来 5-10% 的提升
3. **超参数敏感性**: 大模型需要小学习率和更长的训练
4. **向 SOTA 学习**: 分析第一名代码是最快提升的方法

### 个人成长

通过这个项目，我掌握了：
- Vision Transformer 的原理和应用
- ReID 领域的 SOTA 技术
- 深度学习竞赛的完整流程
- 实验设计和结果分析能力

### 未来方向

1. **模型集成**: 组合多个模型进一步提升
2. **自监督学习**: 探索 MAE, DINO 等方法
3. **边缘设备优化**: 模型蒸馏和量化
4. **实时识别**: 优化推理速度

---

*本文档记录了 Jaguar ReID 竞赛的完整优化历程，可作为保研面试的技术分享材料。*

*最后更新: 2024年*
